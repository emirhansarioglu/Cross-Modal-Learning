{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emirhansarioglu/Cross-Modal-Learning/blob/main/Thesis_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmEdOzW35pr5"
      },
      "source": [
        "# Cross Modal Learning for Scene Classification in Remote Sensing: Thesis Code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "1B_h-UnGKtRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogLgHE7450pt"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "!pip install tqdm\n",
        "import tqdm\n",
        "import rasterio\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from sklearn.metrics import f1_score\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luxJKAlemvAb"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx-2JXPU6hia"
      },
      "source": [
        "## Data Unzip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxVj0aaE6r6s"
      },
      "outputs": [],
      "source": [
        "train_zip_path = '/content/drive/My Drive/Bachelor_Thesis/Dataset/Train_dataset.zip'\n",
        "test_zip_path = '/content/drive/My Drive/Bachelor_Thesis/Dataset/Test_dataset.zip'\n",
        "\n",
        "with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n",
        "with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXEGmWpl62xh"
      },
      "source": [
        "## Custom Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGS_Yyhz7KEp"
      },
      "outputs": [],
      "source": [
        "class SentDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.labels_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.labels_frame.iloc[idx, 0]+'.tif')\n",
        "        with rasterio.open(img_name) as src:\n",
        "                image = src.read()\n",
        "                image = np.transpose(np.array(image), (1,2,0))\n",
        "\n",
        "        labels = self.labels_frame.iloc[idx, 3:].values.astype('float')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            image = image.float()\n",
        "            s2_image = image[2:]\n",
        "            s1_image = image[:2]\n",
        "            labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        return s2_image, s1_image, labels\n",
        "\n",
        "train_csv = '/content/Train_dataset/labels.csv'\n",
        "train_dir = '/content/Train_dataset/images'\n",
        "test_csv = '/content/Test_dataset/labels.csv'\n",
        "test_dir = '/content/Test_dataset/images'\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zE8v1DQ7NLS"
      },
      "outputs": [],
      "source": [
        "Traindataset = SentDataset(csv_file=train_csv, root_dir=train_dir, transform=transform)\n",
        "Trainloader = DataLoader(Traindataset, batch_size=32, shuffle=True)\n",
        "Testdataset = SentDataset(csv_file=test_csv, root_dir=test_dir, transform=transform)\n",
        "Testloader = DataLoader(Testdataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4IMwpYB-hmX"
      },
      "source": [
        "## NN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXxBRqSa7w3Z"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This block includes models that will be used by networks\n",
        "s1_feature_extraction and s2_feature_extraction are modality specific feature vector generators\n",
        "individual_classification model will be used in the cross-modal-focal-loss structure 2 times, 1 for each modality\n",
        "downstream_classfication is the shared part of the network\n",
        "'''\n",
        "\n",
        "class s1_feature_extraction(nn.Module):\n",
        "    def __init__(self, feature_vec_dim = 2048):\n",
        "        super(s1_feature_extraction, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = nn.Linear(256 * 7 * 7, feature_vec_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 256 * 7 * 7)\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class s2_feature_extraction(nn.Module):\n",
        "    def __init__(self, feature_vec_dim = 2048):\n",
        "        super(s2_feature_extraction, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = nn.Linear(256 * 7 * 7, feature_vec_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1, 256 * 7 * 7)\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class individual_classification(nn.Module):\n",
        "    def __init__(self, shared_vec_space_dim=2048):\n",
        "        super(individual_classification, self).__init__()\n",
        "        self.fc1 = nn.Linear(shared_vec_space_dim, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.fc4 = nn.Linear(64, 19)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, feature_s1):\n",
        "        x = self.fc1(feature_s1)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "class downstream_classification(nn.Module):\n",
        "    def __init__(self, shared_vec_space_dim=2048):\n",
        "        super(downstream_classification, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "        self.fc0_5 = nn.Linear(shared_vec_space_dim, shared_vec_space_dim)\n",
        "        self.fc1 = nn.Linear(shared_vec_space_dim, 1024)\n",
        "        self.fc1_5 = nn.Linear(1024, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.fc6 = nn.Linear(64, 32)\n",
        "        self.fc7 = nn.Linear(32, 16)\n",
        "        self.fc8 = nn.Linear(16, 19)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc0_5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc1_5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc6(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc7(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc8(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDNri7sV-mGH"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0ECJk-59kK6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This block contains all helper functions.\n",
        "\n",
        "normalize_to_rgb: range values of array to [0,1] to use matplotlib.imshow\n",
        "init_weights: can be used before training for better convergence\n",
        "cmfl_loss: p and q are the distance of probabilities to wrong label values. Function is assymetric.\n",
        "\n",
        "'''\n",
        "\n",
        "def normalize_to_rgb(arr):\n",
        "    min_val = np.min(arr)\n",
        "    max_val = np.max(arr)\n",
        "    return (arr - min_val) / (max_val - min_val)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def cmfl_loss(p, q, gamma=3.0):\n",
        "    w = q * (2 * p * q) / (p + q)\n",
        "    loss = -1 * (1 - w) ** gamma * torch.log(p)\n",
        "    return loss.mean()\n",
        "\n",
        "def visualize_s2_sample_prediction(Testdataset, device, sentinel2_model, downstream_model, ix):\n",
        "    s2_sample, s1_sample, label_sample = Testdataset[ix]\n",
        "    label_sample = label_sample.to('cpu')\n",
        "    s2_sample = s2_sample.unsqueeze(0).to(device)\n",
        "    feature_s2 = sentinel2_model(s2_sample)\n",
        "    s2_downstream_logits = downstream_model(feature_s2)\n",
        "    prediction = np.array((torch.sigmoid(s2_downstream_logits) > 0.3).float().to('cpu')).squeeze()\n",
        "    short_labels = np.array([\"Urban fabric\", \"Industrial units\", \"Arable land\", \"Permanent crops\", \"Pastures\", \"Complex Cultivation\", \"Agriculture\",\n",
        "                    \"Agro-forestry\", \"Broad-leaved\", \"Coniferous forest\", \"Mixed forest\", \"Grassland\", \"Moors, Heathland\", \"Woodland\", \"Beaches\",\n",
        "                    \"Inland wetlands\", \"Coastal wetlands\", \"Inland waters\", \"Marine waters\"])\n",
        "    predicted_labels = short_labels[prediction == 1]\n",
        "    true_labels = short_labels[np.array(label_sample) == 1]\n",
        "\n",
        "    true_positives = [label for label in predicted_labels if label in true_labels]\n",
        "    false_positives = [label for label in predicted_labels if label not in true_labels]\n",
        "    false_negatives = [label for label in true_labels if label not in predicted_labels]\n",
        "\n",
        "    title_parts = []\n",
        "\n",
        "    for label in true_positives:\n",
        "        title_parts.append((label, 'green'))\n",
        "    for label in false_positives:\n",
        "        title_parts.append((label, 'red'))\n",
        "    for label in false_negatives:\n",
        "        title_parts.append((label, 'blue'))\n",
        "\n",
        "    s2_sample = s2_sample.squeeze().to('cpu')\n",
        "    s2_rgb = np.dstack(np.array([s2_sample[3], s2_sample[2], s2_sample[1]]))\n",
        "\n",
        "    plt.imshow(normalize_to_rgb(s2_rgb))\n",
        "    plt.axis('off')\n",
        "\n",
        "    y_offset = -0.1\n",
        "    for label in true_positives:\n",
        "        plt.text(0.5, y_offset, label, color='green', fontsize=12, ha='center', va='bottom', transform=plt.gca().transAxes)\n",
        "        y_offset -= 0.05\n",
        "\n",
        "    for label in false_positives:\n",
        "        plt.text(0.5, y_offset, label, color='red', fontsize=12, ha='center', va='bottom', transform=plt.gca().transAxes)\n",
        "        y_offset -= 0.05\n",
        "\n",
        "    for label in false_negatives:\n",
        "        plt.text(0.5, y_offset, label, color='blue', fontsize=12, ha='center', va='bottom', transform=plt.gca().transAxes)\n",
        "        y_offset -= 0.05\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhR7Xh0p-4va"
      },
      "source": [
        "## Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc3BHflu-aux"
      },
      "outputs": [],
      "source": [
        "def train_cmfl_model_with_val(sentinel2_model, sentinel1_model,\n",
        "                         s1_classifier, s2_classifier, downstream_model, optimizer,\n",
        "                         dataset, device, val_split=0.2, num_epochs=18, batch_size=32):\n",
        "\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dataset_size = len(dataset)\n",
        "    val_size = int(val_split * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TRAINING\n",
        "        sentinel2_model.train()\n",
        "        sentinel1_model.train()\n",
        "        downstream_model.train()\n",
        "        s1_classifier.train()\n",
        "        s2_classifier.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            images_s2 = images_s2.to(device)\n",
        "            images_s1 = images_s1.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            feature_s1 = sentinel1_model(images_s1)\n",
        "            s1_classifier_preds = s1_classifier(feature_s1)\n",
        "            s1_downstream_logits = downstream_model(feature_s1)\n",
        "            s1_downstream_loss = bce_loss_fn(s1_downstream_logits, labels)\n",
        "            feature_s2 = sentinel2_model(images_s2)\n",
        "            s2_classifier_preds = s2_classifier(feature_s2)\n",
        "            s2_downstream_logits = downstream_model(feature_s2)\n",
        "            s2_downstream_loss = bce_loss_fn(s2_downstream_logits, labels)\n",
        "            shared_network_loss = s1_downstream_loss + s2_downstream_loss\n",
        "            s1 = torch.where(labels == 1, s1_classifier_preds, 1 - s1_classifier_preds)\n",
        "            s2 = torch.where(labels == 1, s2_classifier_preds, 1 - s2_classifier_preds)\n",
        "            cmfl = cmfl_loss(s1, s2) + cmfl_loss(s2, s1)\n",
        "            loss = (shared_network_loss + cmfl)/2\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= 2*len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # VALIDATION\n",
        "        sentinel2_model.eval()\n",
        "        sentinel1_model.eval()\n",
        "        downstream_model.eval()\n",
        "        s1_classifier.eval()\n",
        "        s2_classifier.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images_s2, images_s1, labels = batch\n",
        "                images_s2 = images_s2.to(device)\n",
        "                images_s1 = images_s1.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                feature_s1 = sentinel1_model(images_s1)\n",
        "                s1_classifier_preds = s1_classifier(feature_s1)\n",
        "                s1_downstream_logits = downstream_model(feature_s1)\n",
        "                s1_downstream_loss = bce_loss_fn(s1_downstream_logits, labels)\n",
        "                feature_s2 = sentinel2_model(images_s2)\n",
        "                s2_classifier_preds = s2_classifier(feature_s2)\n",
        "                s2_downstream_logits = downstream_model(feature_s2)\n",
        "                s2_downstream_loss = bce_loss_fn(s2_downstream_logits, labels)\n",
        "                shared_network_loss = s1_downstream_loss + s2_downstream_loss\n",
        "\n",
        "                s1 = torch.where(labels == 1, s1_classifier_preds, 1 - s1_classifier_preds)\n",
        "                s2 = torch.where(labels == 1, s2_classifier_preds, 1 - s2_classifier_preds)\n",
        "                cmfl = cmfl_loss(s1, s2) + cmfl_loss(s2, s1)\n",
        "                loss = (shared_network_loss + cmfl)/2\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= 2*len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_cmfl_model(sentinel2_model, sentinel1_model,\n",
        "                s1_classifier, s2_classifier, downstream_model, optimizer,\n",
        "                dataloader, device, num_epochs=35):\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        sentinel2_model.train()\n",
        "        sentinel1_model.train()\n",
        "        downstream_model.train()\n",
        "        s1_classifier.train()\n",
        "        s2_classifier.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch in tqdm(dataloader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            images_s2 = images_s2.to(device)\n",
        "            images_s1 = images_s1.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            feature_s1 = sentinel1_model(images_s1)\n",
        "            s1_classifier_preds = s1_classifier(feature_s1)\n",
        "            s1_downstream_logits = downstream_model(feature_s1)\n",
        "            s1_downstream_loss = bce_loss_fn(s1_downstream_logits, labels)\n",
        "            feature_s2 = sentinel2_model(images_s2)\n",
        "            s2_classifier_preds = s2_classifier(feature_s2)\n",
        "            s2_downstream_logits = downstream_model(feature_s2)\n",
        "            s2_downstream_loss = bce_loss_fn(s2_downstream_logits, labels)\n",
        "            shared_network_loss = s1_downstream_loss + s2_downstream_loss\n",
        "\n",
        "            s1 = torch.where(labels == 1, s1_classifier_preds, 1 - s1_classifier_preds)\n",
        "            s2 = torch.where(labels == 1, s2_classifier_preds, 1 - s2_classifier_preds)\n",
        "            cmfl = cmfl_loss(s1, s2) + cmfl_loss(s2, s1)\n",
        "            loss = (shared_network_loss + cmfl)/2\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= 2*len(dataloader.dataset)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "def train_model_with_val(sentinel2_model, sentinel1_model, downstream_model, optimizer,\n",
        "                         dataset, device, val_split=0.2, num_epochs=18, batch_size=32):\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    dataset_size = len(dataset)\n",
        "    val_size = int(val_split * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TRAINING\n",
        "        sentinel2_model.train()\n",
        "        sentinel1_model.train()\n",
        "        downstream_model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            images_s2 = images_s2.to(device)\n",
        "            images_s1 = images_s1.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            feature_s1 = sentinel1_model(images_s1)\n",
        "            s1_downstream_logits = downstream_model(feature_s1)\n",
        "            s1_downstream_loss = bce_loss_fn(s1_downstream_logits, labels)\n",
        "            feature_s2 = sentinel2_model(images_s2)\n",
        "            s2_downstream_logits = downstream_model(feature_s2)\n",
        "            s2_downstream_loss = bce_loss_fn(s2_downstream_logits, labels)\n",
        "            shared_network_loss = s1_downstream_loss + s2_downstream_loss\n",
        "            shared_network_loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += shared_network_loss.item()\n",
        "\n",
        "        # VALIDATION\n",
        "        sentinel2_model.eval()\n",
        "        sentinel1_model.eval()\n",
        "        downstream_model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images_s2, images_s1, labels = batch\n",
        "                images_s2 = images_s2.to(device)\n",
        "                images_s1 = images_s1.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                feature_s1 = sentinel1_model(images_s1)\n",
        "                s1_downstream_logits = downstream_model(feature_s1)\n",
        "                s1_downstream_loss = bce_loss_fn(s1_downstream_logits, labels)\n",
        "                feature_s2 = sentinel2_model(images_s2)\n",
        "                s2_downstream_logits = downstream_model(feature_s2)\n",
        "                s2_downstream_loss = bce_loss_fn(s2_downstream_logits, labels)\n",
        "                shared_network_loss = s1_downstream_loss + s2_downstream_loss\n",
        "\n",
        "                val_loss += shared_network_loss.item()\n",
        "\n",
        "        val_loss /= 2*len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        epoch_loss /= 2*len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_model(sentinel2_model, sentinel1_model, downstream_model, optimizer,\n",
        "                dataloader, device, num_epochs=12):\n",
        "\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        sentinel2_model.train()\n",
        "        sentinel1_model.train()\n",
        "        downstream_model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch in tqdm(dataloader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            images_s2 = images_s2.to(device)\n",
        "            images_s1 = images_s1.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            feature_s1 = sentinel1_model(images_s1)\n",
        "            s1_downstream_logits = downstream_model(feature_s1)\n",
        "            s1_downstream_loss = bce_loss_fn(s1_downstream_logits, labels)\n",
        "            feature_s2 = sentinel2_model(images_s2)\n",
        "            s2_downstream_logits = downstream_model(feature_s2)\n",
        "            s2_downstream_loss = bce_loss_fn(s2_downstream_logits, labels)\n",
        "            shared_network_loss = s2_downstream_loss + s1_downstream_loss\n",
        "            shared_network_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += shared_network_loss.item()\n",
        "\n",
        "        epoch_loss /= 2*len(dataloader.dataset)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "def train_single_modality_with_val(sentinel_model, downstream_model, optimizer,\n",
        "                         dataset, device, s2_flag, val_split=0.2, num_epochs=25, batch_size=32):\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dataset_size = len(dataset)\n",
        "    val_size = int(val_split * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TRAINING\n",
        "        sentinel_model.train()\n",
        "        downstream_model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch in tqdm(train_loader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            if s2_flag:\n",
        "                images = images_s2.to(device)\n",
        "            else:\n",
        "                images = images_s1.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            features = sentinel_model(images)\n",
        "            downstream_logits = downstream_model(features)\n",
        "            downstream_loss = bce_loss_fn(downstream_logits, labels)\n",
        "            downstream_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += downstream_loss.item()\n",
        "\n",
        "        epoch_loss /= 2*len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # VALIDATION\n",
        "        sentinel_model.eval()\n",
        "        downstream_model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images_s2, images_s1, labels = batch\n",
        "                if s2_flag:\n",
        "                    images = images_s2.to(device)\n",
        "                else:\n",
        "                    images = images_s1.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                features = sentinel_model(images)\n",
        "                downstream_logits = downstream_model(features)\n",
        "                downstream_loss = bce_loss_fn(downstream_logits, labels)\n",
        "\n",
        "                val_loss += downstream_loss.item()\n",
        "\n",
        "        val_loss /= 2*len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_model_single_modality(sentinel_model, downstream_model, optimizer,\n",
        "                dataloader, device, s2_flag, num_epochs=12):\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        sentinel_model.train()\n",
        "        downstream_model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch in tqdm(dataloader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            if s2_flag:\n",
        "                images = images_s2.to(device)\n",
        "            else:\n",
        "                images = images_s1.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            features = sentinel_model(images)\n",
        "            downstream_logits = downstream_model(features)\n",
        "            downstream_loss = bce_loss_fn(downstream_logits, labels)\n",
        "            downstream_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += downstream_loss.item()\n",
        "\n",
        "        epoch_loss /= 2*len(dataloader.dataset)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "def eval_model(extracter, classifier, data_loader, s2_flag):\n",
        "    extracter.eval()\n",
        "    classifier.eval()\n",
        "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader):\n",
        "            images_s2, images_s1, labels = batch\n",
        "            labels = labels.to(device)\n",
        "            if s2_flag:\n",
        "                images_s2 = images_s2.to(device)\n",
        "                feature = extracter(images_s2)\n",
        "            else:\n",
        "                images_s1 = images_s1.to(device)\n",
        "                feature = extracter(images_s1)\n",
        "            outputs = classifier(feature)\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.3).float()\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "    print(\"all predictions shape: \", all_predictions.shape)\n",
        "    print(\"all labels shape: \", all_labels.shape)\n",
        "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "    f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
        "    print(f'F1-Macro Score: {f1_macro:.3f}')\n",
        "    print(f'F1-Micro Score: {f1_micro:.3f}')\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "    print(\"F1 Scores: \" + \", \".join([f\"{val:.3f}\" for val in f1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcFpTeHsHS5p"
      },
      "source": [
        "## Loading and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22wPM6YNBoww"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "sentinel2_model = s2_feature_extraction().to(device)\n",
        "sentinel1_model = s1_feature_extraction().to(device)\n",
        "downstream_model = downstream_classification().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentinel1_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/S1_S1_extractor.pth'))\n",
        "downstream_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/S1_downstream_model.pth'))\n",
        "eval_model(sentinel1_model, downstream_model, Testloader, s2_flag=False)"
      ],
      "metadata": {
        "id": "0WyFe0xLQf2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D00ijC-mmymD"
      },
      "outputs": [],
      "source": [
        "sentinel1_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/15_S1_S1_extractor.pth'))\n",
        "downstream_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/15_S1_downstream_model.pth'))\n",
        "eval_model(sentinel1_model, downstream_model, Testloader, s2_flag=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentinel2_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/S2_S2_extractor.pth'))\n",
        "downstream_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/S2_downstream_model.pth'))\n",
        "eval_model(sentinel2_model, downstream_model, Testloader, s2_flag=True)"
      ],
      "metadata": {
        "id": "Eur9nfO2Ql1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrVDwza3tPvS"
      },
      "outputs": [],
      "source": [
        "sentinel2_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/15_S2_S2_extractor.pth'))\n",
        "downstream_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/15_S2_downstream_model.pth'))\n",
        "eval_model(sentinel2_model, downstream_model, Testloader, s2_flag=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFBFbOs0HSaL"
      },
      "outputs": [],
      "source": [
        "visualize_s2_sample_prediction(Testdataset, device, sentinel2_model, downstream_model, 12000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiE4tYkUjySZ"
      },
      "outputs": [],
      "source": [
        "sentinel1_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/20_CM_S1_extractor.pth'))\n",
        "sentinel2_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/20_CM_S2_extractor.pth'))\n",
        "downstream_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/CM/20_CM_downstream_model.pth'))\n",
        "eval_model(sentinel1_model, downstream_model, Testloader, s2_flag=False)\n",
        "print(\"\\nNow S2 \\n\")\n",
        "eval_model(sentinel2_model, downstream_model, Testloader, s2_flag=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVhpD0vxoPEl"
      },
      "outputs": [],
      "source": [
        "sentinel1_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/N_CMFL/S1_extractor.pth'))\n",
        "sentinel2_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/N_CMFL/S2_extractor.pth'))\n",
        "downstream_model.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/N_CMFL/downstream_model.pth'))\n",
        "eval_model(sentinel1_model, downstream_model, Testloader, s2_flag=False)\n",
        "print(\"\\nNow S2 \\n\")\n",
        "eval_model(sentinel2_model, downstream_model, Testloader, s2_flag=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uvpoy_MHtb1"
      },
      "outputs": [],
      "source": [
        "#testing a single training step\n",
        "for i, batch in enumerate(Testloader):\n",
        "      if i == 1:\n",
        "        break\n",
        "      else:\n",
        "          images_s2, images_s1, labels = batch\n",
        "          images_s2 = images_s2.to(device)\n",
        "          images_s1 = images_s1.to(device)\n",
        "          labels = labels.to(device)\n",
        "          print(\"S1 shape: \", images_s1.shape)\n",
        "          print(\"S2 shape: \", images_s2.shape)\n",
        "          print(\"Labels shape: \", labels.shape)\n",
        "\n",
        "          s1_sample = images_s1[30]\n",
        "          s1_sample = s1_sample.squeeze().to('cpu')\n",
        "          s1_rgb = np.dstack(np.array([s1_sample[0], s1_sample[1], s1_sample[0]- s1_sample[1]]))\n",
        "          s2_sample = images_s2[30]\n",
        "          s2_sample = s2_sample.squeeze().to('cpu')\n",
        "          s2_rgb = np.dstack(np.array([s2_sample[3], s2_sample[2], s2_sample[1]]))\n",
        "\n",
        "          fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "          axes[0].imshow(normalize_to_rgb(s1_rgb))\n",
        "          axes[0].set_title('Sentinel-1')\n",
        "          axes[0].axis('off')\n",
        "\n",
        "          axes[1].imshow(normalize_to_rgb(s2_rgb))\n",
        "          axes[1].set_title('Sentinel-2')\n",
        "          axes[1].axis('off')\n",
        "\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIC55gBitCbL"
      },
      "source": [
        "\n",
        "## Training and Evaluating Cross Modal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW7rrrswtRme"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "sentinel2_model = s2_feature_extraction().to(device)\n",
        "sentinel1_model = s1_feature_extraction().to(device)\n",
        "downstream_model = downstream_classification().to(device)\n",
        "sentinel2_model.apply(init_weights)\n",
        "sentinel1_model.apply(init_weights)\n",
        "downstream_model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(list(sentinel2_model.parameters()) +\n",
        "                       list(sentinel1_model.parameters()) +\n",
        "                       list(downstream_model.parameters()), lr=0.001, weight_decay=0.00001)\n",
        "s1_optimizer = optim.Adam(list(sentinel1_model.parameters()) +\n",
        "                       list(downstream_model.parameters()), lr=0.001, weight_decay=0.00001)\n",
        "s2_optimizer = optim.Adam(list(sentinel2_model.parameters()) +\n",
        "                       list(downstream_model.parameters()), lr=0.001, weight_decay=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOAAHs1RtzR7"
      },
      "outputs": [],
      "source": [
        "train_single_modality_with_val(sentinel1_model, downstream_model, s1_optimizer,\n",
        "                            Traindataset, device, s2_flag=False, num_epochs = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCBiibTzXQKp"
      },
      "outputs": [],
      "source": [
        "train_model_single_modality(sentinel1_model, downstream_model, s1_optimizer,\n",
        "                            Trainloader, device, s2_flag=False, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HddW4QTrt8p3"
      },
      "outputs": [],
      "source": [
        "train_single_modality_with_val(sentinel2_model, downstream_model, s2_optimizer,\n",
        "                            Traindataset, device, s2_flag=True, num_epochs = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV4MGgUYo_8C"
      },
      "outputs": [],
      "source": [
        "train_model_single_modality(sentinel2_model, downstream_model, s2_optimizer,\n",
        "                            Trainloader, device, s2_flag=True, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks06jvN_eEE_"
      },
      "outputs": [],
      "source": [
        "visualize_s2_sample_prediction(Testdataset, device, sentinel2_model, downstream_model, 500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_val(sentinel2_model, sentinel1_model, downstream_model, optimizer,\n",
        "                     Traindataset, device, val_split=0.2, num_epochs=40, batch_size=32)"
      ],
      "metadata": {
        "id": "ZtjBI61SP-1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(sentinel2_model, sentinel1_model, downstream_model, optimizer,\n",
        "            Trainloader, device, num_epochs=30)"
      ],
      "metadata": {
        "id": "RzBIYILx-2OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_HQwvi_OK4V"
      },
      "outputs": [],
      "source": [
        "visualize_s2_sample_prediction(Testdataset, device, sentinel2_model, downstream_model, 600)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(sentinel1_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S1_extractor.pth')\n",
        "torch.save(sentinel2_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S2_extractor.pth')\n",
        "#torch.save(downstream_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S1_downstream_model.pth')\n",
        "torch.save(sentinel2_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S2_downstream_model.pth')"
      ],
      "metadata": {
        "id": "1XOb8LuiHn4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYqRMWut8nCf"
      },
      "outputs": [],
      "source": [
        "torch.save(sentinel1_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_S1_extractor.pth')\n",
        "torch.save(sentinel2_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_S2_extractor.pth')\n",
        "torch.save(downstream_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_downstream_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onlib_b3Ii6s"
      },
      "source": [
        "## Training and Evaluating Cross Modal with Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCb5HIn1IrOV"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "sentinel2_model = s2_feature_extraction().to(device)\n",
        "sentinel1_model = s1_feature_extraction().to(device)\n",
        "s1_classifier = individual_classification().to(device)\n",
        "s2_classifier = individual_classification().to(device)\n",
        "downstream_model = downstream_classification().to(device)\n",
        "sentinel2_model.apply(init_weights)\n",
        "sentinel1_model.apply(init_weights)\n",
        "downstream_model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(list(sentinel2_model.parameters()) +\n",
        "                       list(sentinel1_model.parameters()) +\n",
        "                       list(s1_classifier.parameters()) +\n",
        "                       list(s2_classifier.parameters()) +\n",
        "                       list(downstream_model.parameters()), lr=0.001, weight_decay=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2k9gzuOItdt"
      },
      "outputs": [],
      "source": [
        "train_cmfl_model_with_val(sentinel2_model, sentinel1_model, s1_classifier, s2_classifier, downstream_model, optimizer,\n",
        "                     Traindataset, device, val_split=0.2, num_epochs=40, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiXwI3H_CYh-"
      },
      "outputs": [],
      "source": [
        "train_cmfl_model(sentinel2_model, sentinel1_model, s1_classifier, s2_classifier, downstream_model, optimizer,\n",
        "            Trainloader, device, num_epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn1FnrakOYJ8"
      },
      "outputs": [],
      "source": [
        "visualize_s2_sample_prediction(Testdataset, device, sentinel2_model, downstream_model, 600)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(sentinel1_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CMFL_S1_extractor.pth')\n",
        "torch.save(sentinel2_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CMFL_S2_extractor.pth')\n",
        "torch.save(downstream_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CMFL_downstream_model.pth')\n",
        "torch.save(sentinel1_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_s1_classifier.pth')\n",
        "torch.save(sentinel2_model.state_dict(), '/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_s2_classifier.pth')"
      ],
      "metadata": {
        "id": "X7GiLTRyIFXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UMAP"
      ],
      "metadata": {
        "id": "gv8gbjiOtKMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "zCKSFE12upIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract Features and Labels (currently for cross-modal feature extraction models)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "s2_feature_extractor = s2_feature_extraction().to(device)\n",
        "s2_feature_extractor.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_S2_extractor.pth'))\n",
        "s1_feature_extractor = s1_feature_extraction().to(device)\n",
        "s1_feature_extractor.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_S1_extractor.pth'))\n",
        "\n",
        "s1_features = []\n",
        "s2_features = []\n",
        "labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in Trainloader:\n",
        "        images_s2, images_s1, targets = batch\n",
        "        images_s1 = images_s1.to(device)\n",
        "        images_s2 = images_s2.to(device)\n",
        "        features_s1 = s1_feature_extractor(images_s1)\n",
        "        features_s2 = s2_feature_extractor(images_s2)\n",
        "        s1_features.append(features_s1.cpu().numpy())\n",
        "        s2_features.append(features_s2.cpu().numpy())\n",
        "        labels.append(targets.cpu().numpy())\n",
        "\n",
        "s1_features = np.concatenate(s1_features)  # Shape: (num_samples, 2048)\n",
        "s2_features = np.concatenate(s2_features)\n",
        "labels = np.concatenate(labels)  # Shape: (num_samples, 19)"
      ],
      "metadata": {
        "id": "_F_v4grml12u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_points_s1 = s1_features[labels[:, 18] == 1] # 18: Marine Waters\n",
        "class_points_s2 = s2_features[labels[:, 18] == 1]\n",
        "print(class_points_s1.shape)\n",
        "print(class_points_s2.shape)\n",
        "all_class_points = np.concatenate([class_points_s1, class_points_s2], axis = 0)\n",
        "print(all_class_points.shape)\n",
        "\n",
        "# Step 2: Perform UMAP on Entire Dataset\n",
        "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
        "all_embeddings = umap_model.fit_transform(all_class_points)\n",
        "s1_embeddings = all_embeddings[:class_points_s1.shape[0]]\n",
        "s2_embeddings = all_embeddings[class_points_s1.shape[0]:]\n",
        "print(\"2-d Marine Waters s1 feature vector count: \", s1_embeddings.shape)\n",
        "print(\"2-d Marine Waters s2 feature vector count: \", s2_embeddings.shape)"
      ],
      "metadata": {
        "id": "YB32ebOh6f0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(s1_embeddings[:, 0], s1_embeddings[:, 1], c = \"blue\", s=10, alpha=0.7, label = \"Sentinel-1\")\n",
        "plt.scatter(s2_embeddings[:, 0], s2_embeddings[:, 1], c= \"orange\", s=10, alpha=0.7, label = \"Sentinel-2\")\n",
        "plt.legend(markerscale = 3, loc = \"lower right\")\n",
        "plt.title(\"Marine Waters Class in Cross-Modal Network\")\n",
        "plt.xlim(min(s1_embeddings[:, 0].min(), s2_embeddings[:, 0].min()) - 1, max(s1_embeddings[:, 0].max(), s2_embeddings[:, 0].max()) + 1)\n",
        "plt.ylim(min(s1_embeddings[:, 1].min(), s2_embeddings[:, 1].min()) - 1, max(s1_embeddings[:, 1].max(), s2_embeddings[:, 1].max()) + 1)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a4sovRHk3Xdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract Features and Labels (change the path for other networks)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "baseline_feature_extractor = s2_feature_extraction().to(device)\n",
        "baseline_feature_extractor.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S2_extractor.pth'))\n",
        "\n",
        "baseline_features = []\n",
        "labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in Trainloader:\n",
        "        images_s2, images_s1, targets = batch\n",
        "        images_s2 = images_s2.to(device)\n",
        "        features = baseline_feature_extractor(images_s2)\n",
        "        baseline_features.append(features.cpu().numpy())\n",
        "        labels.append(targets.cpu().numpy())\n",
        "\n",
        "baseline_features = np.concatenate(baseline_features)\n",
        "labels = np.concatenate(labels)\n",
        "\n",
        "# Step 2: Perform UMAP on Entire Dataset\n",
        "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
        "features_2d = umap_model.fit_transform(baseline_features)\n"
      ],
      "metadata": {
        "id": "YCkSxDY7aeUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = labels.shape[1]\n",
        "palette = sns.color_palette(\"hsv\", num_classes)  # Generate colors for each class"
      ],
      "metadata": {
        "id": "-Zjn883tn3ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_points_0 = features_2d[labels[:, 0] == 1]  # 2D embeddings for the selected class Urban Fabric\n",
        "class_points_18 = features_2d[labels[:, 18] == 1]  # Marine Waters\n",
        "class_points_8 = features_2d[labels[:, 8] == 1] # Broad Leaved Forest\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(class_points_8[:, 0], class_points_8[:, 1], c=[palette[8]], s=10, alpha=0.7, label = \"Broad Leaved Forest\")\n",
        "plt.scatter(class_points_18[:, 0], class_points_18[:, 1], c=[palette[18]], s=10, alpha=0.7, label = \"Marine Waters\")\n",
        "plt.scatter(class_points_0[:, 0], class_points_0[:, 1], c=[palette[0]], s=10, alpha=0.7, label = \"Urban Fabric\")\n",
        "plt.legend(markerscale = 3)\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.xlim(features_2d[:, 0].min() - 1, features_2d[:, 0].max() + 1) ### ensure same coordinate for each class\n",
        "plt.ylim(features_2d[:, 1].min() - 1, features_2d[:, 1].max() + 1)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qftaAUQ2zGIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_points_16 = features_2d[labels[:, 16] == 1]  # Coastal Wetlands\n",
        "class_points_10 = features_2d[labels[:, 10] == 1]  # Mixed Forest\n",
        "class_points_2 = features_2d[labels[:, 2] == 1] # Arable Land\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(class_points_2[:, 0], class_points_2[:, 1], c=[palette[2]], s=10, alpha=0.7, label = \"Arable Land\")\n",
        "plt.scatter(class_points_10[:, 0], class_points_10[:, 1], c=[palette[10]], s=10, alpha=0.7, label = \"Mixed Forest\")\n",
        "plt.scatter(class_points_16[:, 0], class_points_16[:, 1], c=[palette[16]], s=10, alpha=0.7, label = \"Coastal Wetlands\")\n",
        "plt.legend(markerscale = 3, loc = \"lower right\")\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.xlim(features_2d[:, 0].min() - 1, features_2d[:, 0].max() + 1) ### ensure same coordinate for each class\n",
        "plt.ylim(features_2d[:, 1].min() - 1, features_2d[:, 1].max() + 1)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P4xDAsRJgrE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pearson Correlation"
      ],
      "metadata": {
        "id": "HpoWRg845Eq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Pearson correlation for two vectors (per-sample)\n",
        "def pearson_correlation(x, y):\n",
        "    mean_x = x.mean()\n",
        "    mean_y = y.mean()\n",
        "    numerator = torch.sum((x - mean_x) * (y - mean_y))\n",
        "    denominator = torch.sqrt(torch.sum((x - mean_x) ** 2) * torch.sum((y - mean_y) ** 2))\n",
        "    return numerator / denominator\n",
        "\n",
        "# BASELINE\n",
        "feature_extractor_s2 = s2_feature_extraction().to(device)\n",
        "feature_extractor_s2.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S2_extractor.pth'))\n",
        "feature_extractor_s1 = s1_feature_extraction().to(device)\n",
        "feature_extractor_s1.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S1_extractor.pth'))\n",
        "s2_classifier = downstream_classification().to(device)\n",
        "s2_classifier.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S2_downstream_model.pth'))\n",
        "s1_classifier = downstream_classification().to(device)\n",
        "s1_classifier.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/S1_downstream_model.pth'))\n",
        "\n",
        "# Initialize a list to store per-sample correlations\n",
        "per_sample_correlations = []\n",
        "\n",
        "# Loop through the test dataset\n",
        "for batch in Testloader:\n",
        "    with torch.no_grad():\n",
        "        # Get images from the batch\n",
        "        images_s2, images_s1, _ = batch\n",
        "        images_s2 = images_s2.to(device)\n",
        "        images_s1 = images_s1.to(device)\n",
        "\n",
        "        features_s1 = feature_extractor_s1(images_s1)\n",
        "        preds_s1 = s1_classifier(features_s1) # Predictions from Model 1 (for Sentinel-1)\n",
        "        features_s2 = feature_extractor_s2(images_s2)\n",
        "        preds_s2 = s2_classifier(features_s2)  # Predictions from Model 2 (for Sentinel-2)\n",
        "\n",
        "        # Compute correlation for each sample in the batch\n",
        "        for i in range(preds_s1.shape[0]):\n",
        "            sample_preds_s1 = preds_s1[i]\n",
        "            sample_preds_s2 = preds_s2[i]\n",
        "\n",
        "            correlation = pearson_correlation(sample_preds_s1, sample_preds_s2)\n",
        "            per_sample_correlations.append(correlation.item())  # Store the correlation for this sample\n",
        "\n",
        "# Average the per-sample correlations\n",
        "average_correlation = torch.mean(torch.tensor(per_sample_correlations))\n",
        "\n",
        "print(f\"Average Pearson Correlation between model predictions: {average_correlation.item()}\")"
      ],
      "metadata": {
        "id": "trjGHuPd-NPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Modal\n",
        "feature_extractor_s2 = s2_feature_extraction().to(device)\n",
        "feature_extractor_s2.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_S2_extractor.pth'))\n",
        "feature_extractor_s1 = s1_feature_extraction().to(device)\n",
        "feature_extractor_s1.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_S1_extractor.pth'))\n",
        "classifier = downstream_classification().to(device)\n",
        "classifier.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights/CM_downstream_model.pth'))\n",
        "\n",
        "\n",
        "# Initialize a list to store per-sample correlations\n",
        "per_sample_correlations = []\n",
        "\n",
        "# Loop through the test dataset\n",
        "for batch in Testloader:\n",
        "    with torch.no_grad():\n",
        "        # Get images from the batch\n",
        "        images_s2, images_s1, _ = batch\n",
        "        images_s2 = images_s2.to(device)\n",
        "        images_s1 = images_s1.to(device)\n",
        "\n",
        "        features_s1 = feature_extractor_s1(images_s1)\n",
        "        preds_s1 = classifier(features_s1) # Predictions from Model 1 (for Sentinel-1)\n",
        "        features_s2 = feature_extractor_s2(images_s2)\n",
        "        preds_s2 = classifier(features_s2)  # Predictions from Model 2 (for Sentinel-2)\n",
        "\n",
        "        # Compute correlation for each sample in the batch\n",
        "        for i in range(preds_s1.shape[0]):\n",
        "            sample_preds_s1 = preds_s1[i]\n",
        "            sample_preds_s2 = preds_s2[i]\n",
        "\n",
        "            correlation = pearson_correlation(sample_preds_s1, sample_preds_s2)\n",
        "            per_sample_correlations.append(correlation.item())  # Store the correlation for this sample\n",
        "\n",
        "# Average the per-sample correlations\n",
        "average_correlation = torch.mean(torch.tensor(per_sample_correlations))\n",
        "\n",
        "print(f\"Average Pearson Correlation between model predictions: {average_correlation.item()}\")\n"
      ],
      "metadata": {
        "id": "ofgx1Ad5BLO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Modal Focal Loss\n",
        "feature_extractor_s2 = s2_feature_extraction().to(device)\n",
        "feature_extractor_s2.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights2/CMFL_S2_extractor.pth'))\n",
        "feature_extractor_s1 = s1_feature_extraction().to(device)\n",
        "feature_extractor_s1.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights2/CMFL_S1_extractor.pth'))\n",
        "classifier = downstream_classification().to(device)\n",
        "classifier.load_state_dict(torch.load('/content/drive/My Drive/Bachelor_Thesis/Model/Model_Weights2/CMFL_downstream_model.pth'))\n",
        "\n",
        "# Initialize a list to store per-sample correlations\n",
        "per_sample_correlations = []\n",
        "\n",
        "# Loop through the test dataset\n",
        "for batch in Testloader:\n",
        "    with torch.no_grad():\n",
        "        # Get images from the batch\n",
        "        images_s2, images_s1, label = batch\n",
        "        images_s2 = images_s2.to(device)\n",
        "        images_s1 = images_s1.to(device)\n",
        "\n",
        "        features_s1 = feature_extractor_s1(images_s1)\n",
        "        preds_s1 = classifier(features_s1) # Predictions from Model 1 (for Sentinel-1)\n",
        "        features_s2 = feature_extractor_s2(images_s2)\n",
        "        preds_s2 = classifier(features_s2)  # Predictions from Model 2 (for Sentinel-2)\n",
        "\n",
        "        # Compute correlation for each sample in the batch\n",
        "        for i in range(preds_s1.shape[0]):\n",
        "            sample_preds_s1 = preds_s1[i]\n",
        "            sample_preds_s2 = preds_s2[i]\n",
        "\n",
        "            correlation = pearson_correlation(sample_preds_s1, sample_preds_s2)\n",
        "            per_sample_correlations.append(correlation.item())  # Store the correlation for this sample\n",
        "\n",
        "# Average the per-sample correlations\n",
        "average_correlation = torch.mean(torch.tensor(per_sample_correlations))\n",
        "\n",
        "print(f\"Average Pearson Correlation between model predictions: {average_correlation.item()}\")"
      ],
      "metadata": {
        "id": "EXZla2rDC8kY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOD71j2bb8AhIzn0zo4SD+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}